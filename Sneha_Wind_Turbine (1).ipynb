{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sneha_Wind_Turbine.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3YB3oRIPjnIt",
        "pbyc9OlkOnk4",
        "KzrK5WxARgfi",
        "RCkm2R-XRg7X"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **WIND-TURBINE POWER PREDICTION (MACHINE LEARNING - ADVANCED)**\n",
        "\n",
        "**Submitted By:**\n",
        "\n",
        "**SNEHA KULKARNI**\n",
        "\n",
        "**INSAID- GCD, July 18th 2021 Cohort**\n"
      ],
      "metadata": {
        "id": "3YB3oRIPjnIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Table of Contents**\n",
        "---\n",
        "\n",
        "1. [**Introduction**](#Section1)<br>\n",
        "2. [**Problem Statement**](#Section2)<br>\n",
        "3. [**Installing & Importing Libraries**](#Section3)<br>\n",
        "  3.1 [**Installing Libraries**](#Section31)<br>\n",
        "  3.3 [**Importing Libraries**](#Section33)<br>\n",
        "4. [**Data Acquisition & Description**](#Section4)<br>\n",
        "5. [**Data Pre-Profiling**](#Section5)<br>\n",
        "6. [**Data Pre-Processing**](#Section6)<br>\n",
        "7. [**Data Post-Profiling**](#Section7)<br>\n",
        "8. [**Exploratory Data Analysis**](#Section8)<br>\n",
        "9. [**Splitting data into training and test**](#Section9)</br>\n",
        "10. [**Scaling**](#Section10)</br>\n",
        "11. [**Model Building**](#Section11)</br>\n",
        "12. [**Predicting Unseen Data**](#Section12)</br>\n",
        "13. [**Preparing Submission File**](#Section13)</br>\n",
        "14. [**Summary**](#Section14)</br>\n"
      ],
      "metadata": {
        "id": "OpmEf6pBTV9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section1></a>\n",
        "# **1. Introduction**\n",
        "\n",
        "---\n",
        "\n",
        " Company Introduction - Energy Limited\n",
        "\n",
        " Your client for this project is a Renewable energy institution.\n",
        "\n",
        "  1) They are going to provide an amount of power generated from a wind turbine in KW/hr by using the real time data.\n",
        "\n",
        "  2) Factors such as temperature, wind direction, turbine status, weather, blade length, etc. influence the amount of power generated.\n",
        "\n",
        "  3) We have to select the most important features which help us to generate more power in an efficient way.\n",
        "\n",
        "### Current Scenario\n",
        " -  Company rolled out this service to several areas and they will monitor which features can increase the power generated by the turbines. Using this they can map those areas for future investments.\n"
      ],
      "metadata": {
        "id": "We_wd_hhOcHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section2></a>\n",
        "# **2. Problem Statement**\n",
        "\n",
        "---\n",
        "\n",
        "Moving from traditional energy plans powered by fossil fuels to unlimited renewable energy subscriptions allows for instant access to clean energy without heavy investment in infrastructure like Wind Turbines.\n",
        "\n",
        "### The current process suffers from the following problems:\n",
        "- 1) One issue is that **spinning turbine** blades can pose a threat to flying wildlife like birds and bats.\n",
        "- 2) Wind energy can have adverse environmental impacts, including the potential to reduce, fragment, or degrade habitat for wildlife, fish, and plants.\n",
        "- 3) The company wants to figure out how they can manage these challenges to produce wind energy in an efficient manner.\n",
        "\n",
        "The energy department has hired you as data science consultants.\n",
        "\n",
        "### Your Role\n",
        "- You are given datasets of wind turbines and the power generated by them.\n",
        "- Your task is to build a regression model using the datasets.\n",
        "- Because there was no machine learning model for this problem in the company, you donâ€™t have a quantifiable win condition. You need to build the best possible model.\n",
        "\n",
        "### Project Deliverable\n",
        "   - Deliverable: **Deliverable: Predict the power that is generated (in KW/h) based on the various features provided in the dataset.**\n",
        "   - Machine Learning Task: **Regression**\n",
        "   - Target Variable: **windmill_generated_power(kW/h)**\n",
        "   - Win Condition: **N/A (best possible model)**\n",
        "\n",
        "### Evaluation Metric\n",
        "   - The model evaluation will be based on the **r2** Score.\n",
        "\n"
      ],
      "metadata": {
        "id": "pbyc9OlkOnk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section3></a>\n",
        "# **3. Installing and Importing Libraries**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xDrddiESOvLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "<a name = Section31></a>\n",
        "## **3.1 Installing Libraries**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "a_J4hkoEOv_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datascience                   # Package that is required by pandas profiling\n",
        "!pip install -q pandas-profiling              # Library to generate basic statistics about data\n",
        "!pip install -q --upgrade pandas-profiling"
      ],
      "metadata": {
        "id": "NvqhWh8JmtOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install MarkupSafe==2.0.1"
      ],
      "metadata": {
        "id": "ojQoLv9tmwYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section32></a>\n",
        "## **3.1 Importing Libraries**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0yUDs0PKOwg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import pandas as pd                                                 # Importing for panel data analysis\n",
        "pd.set_option('display.max_columns', None)                          # Unfolding hidden features if the cardinality is high\n",
        "pd.set_option('display.max_colwidth', None)                         # Unfolding the max feature width for better clearity\n",
        "pd.set_option('display.max_rows', None)                             # Unfolding hidden data points if the cardinality is high\n",
        "pd.set_option('mode.chained_assignment', None)                      # Removing restriction over chained assignments operations\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)         # To suppress scientific notation over exponential values\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import numpy as np                                                  # Importing package numpys (For Numerical Python)\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import matplotlib.pyplot as plt                                     # Importing pyplot interface using matplotlib\n",
        "import seaborn as sns                                               # Importin seaborm library for interactive visualization\n",
        "import plotly.graph_objs as go                                      # Importing plotly for interactive visualizations\n",
        "%matplotlib inline\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "MNWMrlFkm2G2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import pandas as pd                                                 # Importing for panel data analysis\n",
        "pd.set_option('display.max_columns', None)                          # Unfolding hidden features if the cardinality is high\n",
        "pd.set_option('display.max_colwidth', None)                         # Unfolding the max feature width for better clearity\n",
        "pd.set_option('display.max_rows', None)                             # Unfolding hidden data points if the cardinality is high\n",
        "pd.set_option('mode.chained_assignment', None)                      # Removing restriction over chained assignments operations\n",
        "pd.set_option('display.float_format', lambda x: '%.5f' % x)         # To suppress scientific notation over exponential values\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import numpy as np                                                  # Importing package numpys (For Numerical Python)\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import matplotlib.pyplot as plt                                     # Importing pyplot interface using matplotlib\n",
        "import seaborn as sns                                               # Importin seaborm library for interactive visualization\n",
        "import plotly.graph_objs as go                                      # Importing plotly for interactive visualizations\n",
        "%matplotlib inline\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "from sklearn.preprocessing import StandardScaler                   # Importing to scale the features in the dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split                # To properly split the dataset into train and test sets\n",
        "from sklearn.linear_model import LogisticRegression                   # To create a linear regression model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier                  # To create a random forest regressor model\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB                          # To create a naive bayes model using algorithm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import metrics                                         # Importing to evaluate the model used for regression\n",
        "from sklearn.decomposition import PCA                               # Importing to create an instance of PCA model\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "from random import randint                                          # Importing to generate random integers\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "import time                                                         # For time functionality\n",
        "import warnings                                                     # Importing warning to disable runtime warnings\n",
        "warnings.filterwarnings(\"ignore\")                                   # Warnings will appear only once\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "!pip install imblearn\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neMMmmyZm4Tu",
        "outputId": "54301920-663a-452c-de48-95c92e821cd8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (from imblearn) (0.8.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section4></a>\n",
        "# **4 Data acquisition and Description**\n"
      ],
      "metadata": {
        "id": "sVfrU_pxOw7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|***ID***|****Feature****|****Description****           |\n",
        "|:--|:--|:--|\n",
        "|01| tracking_id       | Represents a unique identification number of a windmill.|\n",
        "|02| datetime  | Represents the date and time of a record.|  \n",
        "|03| wind_speed(m/s)       | Represents the speed of wind (in meter per second).| \n",
        "|04| atmospheric_temperature(Â°C)    | Represents the temperature (in degree Celsius) of a town or village that the windmill is present in.|   \n",
        "|05| shaft_temperature(Â°C)    | Represents the temperature of the shaft (in degree Celsius). |\n",
        "|06| blades_angle(Â°)       | Represents the angle of the blades of a wind turbine (in degrees).|\n",
        "|07| gearbox_temperature(Â°C)     | Represents the temperature of a gearbox (in degree Celsius).|\n",
        "|08| engine_temperature(Â°C) | Represents the temperature of an engine (in degree Celsius).|\n",
        "|09| motor_torque(N-m) | Represents the torque of a motor (in Newton meter).|\n",
        "|10| generator_temperature(Â°C) | Represents the temperature of a generator (in degree Celsius).|\n",
        "|11| atmospheric_pressure(Pascal) | Represents the atmospheric pressure (in Pascals) in that area.|\n",
        "|12| area_temperature(Â°C)      | Represents the temperature (in degree Celsius) of the area within a 100 m radius of the windmill.|\n",
        "|13| windmill_body_temperature(Â°C) | Represents the temperature of the body of a windmill (in degree Celsius).|\n",
        "|14| wind_direction(Â°) | Represents the direction of the wind (in degrees).|\n",
        "|15| resistance(ohm) | Represents the resistance against the wind.|\n",
        "|16| rotor_torque(N-m) | Represents the torque of a rotor (in Newton meter).|\n",
        "|17| turbine_status | Represents the torque of a rotor (in Newton meter).|\n",
        "|18| cloud_level | Represents the following levels of the cloud in the sky on a particular day: Extremely low, Low, Medium.|\n",
        "|19| blade_length(m) | Represents the length of the blades of a windmill (in meter)|\n",
        "|20| blade_breadth(m) | Represents the breadth of the blades of a windmill (in meter).|\n",
        "|21| windmill_height(m) | Represents the height of the blades of a windmill (in meter).|\n",
        "|22| windmill_generated_power(kW/h)| Represents the power generated (in Kilowatt per hour)|\n"
      ],
      "metadata": {
        "id": "dFE386BKpiOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqXVZorAeFX1",
        "outputId": "2b6a8a72-de54-45f7-a2b8-377e6c391910"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine = pd.read_csv(\"/content/drive/MyDrive/Windturbine_train.csv\")"
      ],
      "metadata": {
        "id": "DtNMFhZYngNY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.head()"
      ],
      "metadata": {
        "id": "mtUmVDVtpK-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.shape"
      ],
      "metadata": {
        "id": "DI2G8DDQpVg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.info()"
      ],
      "metadata": {
        "id": "Q8VUrHgzpa3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.describe()"
      ],
      "metadata": {
        "id": "tAUTU6zUpajw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section5></a>\n",
        "# **5 Data Pre-Profiling**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yPDQsgBgOxWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas profiling\n",
        "\n",
        "from pandas_profiling import ProfileReport                          # Import Pandas Profiling (To generate Univariate Analysis) \n",
        "windTurbine_profile=ProfileReport(df_windTurbine)\n",
        "windTurbine_profile"
      ],
      "metadata": {
        "id": "LktnC-earcgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section6></a>\n",
        "# **6 Data Preprocessing**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JvMmdi1gOxvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking for Duplicates"
      ],
      "metadata": {
        "id": "MMM1txrpPqNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to check duplicates in the  dataset\n",
        "\n",
        "def duplicate_values(df):\n",
        "    print(\"Duplicate check...\")\n",
        "    print(\"There are\", df.duplicated().sum(), \"duplicate observations in the dataset.\")\n",
        "\n",
        "    # dup = df.groupby('ID').size()\n",
        "    # dup = dup[dup>1]\n",
        "    # print(\"Duplicate ID  : {} \".format(dup))\n",
        "\n",
        "    duplicate_values = df.duplicated().sum()\n",
        "    if duplicate_values > 0:\n",
        "        df.drop_duplicates(keep = 'first', inplace = True)\n",
        "        print(duplicate_values,(\" Duplicates were dropped!\"))\n",
        "    else:\n",
        "        print(\"There are no duplicates\")  \n"
      ],
      "metadata": {
        "id": "aMgt2hrV4VZu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_values(df_windTurbine)"
      ],
      "metadata": {
        "id": "n0OeDt354WcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking for Missing Values"
      ],
      "metadata": {
        "id": "y2abGbdiPq7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.isnull().sum().sum()"
      ],
      "metadata": {
        "id": "Z9Gs3ohS4zlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to return missing values in the data\n",
        "\n",
        "def missing_values(df):\n",
        "    missing_number = df.isnull().sum().sort_values(ascending = False)\n",
        "    missing_percent = (df.isnull().sum() / df.isnull().count()).sort_values(ascending = False)\n",
        "    missing_values = pd.concat([missing_number, missing_percent], axis = 1, keys = ['Missing_Number', 'Missing_Percent'])\n",
        "    return missing_values[missing_values['Missing_Number'] > 0]"
      ],
      "metadata": {
        "id": "dbw5rArd4es8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values(df_windTurbine)"
      ],
      "metadata": {
        "id": "pyZgP2Oi4j2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the missing data\n",
        "\n",
        "def plot_missing_data(df):\n",
        "\n",
        "  missing_percent = 100 * df.isnull().sum() / len(df)\n",
        "  missing_percent = missing_percent[missing_percent > 0]\n",
        "  missing_percent.sort_values(inplace = True)\n",
        "\n",
        "  df_missing = pd.DataFrame(missing_percent)\n",
        "\n",
        "  missing_percent.plot.bar()\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "E-Cs1DUS-Y6j"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_missing_data(df_windTurbine)"
      ],
      "metadata": {
        "id": "DZjOBHmS-c4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print value counts for all 'objects' with more than 1 null value\n",
        "# This provide an output that shows the values of each categorical feature that has null values\n",
        "def object_vcs_and_nulls(df):\n",
        "  for i in df:\n",
        "    if df[i].dtype == 'O':\n",
        "      if df[i].isnull().sum() > 0:\n",
        "        print(df[i].value_counts())  \n",
        "        print(\"Number of Null Values: \" + str(df[i].isnull().sum()))\n",
        "        print(\"Percentage of Nulls = \" + str(np.round((df[i].isnull().sum() / 14.60), 2)) + \"%\")\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "LTmPc2T_-xrC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_vcs_and_nulls(df_windTurbine)"
      ],
      "metadata": {
        "id": "5OngAy8y-9S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check ALL the NUMERICAL COLUMNS for -99 values and Replace/Substitute them with NaN values\n",
        "\n",
        "(df_windTurbine == -99 ).sum(axis = 0)"
      ],
      "metadata": {
        "id": "B10TQllzV0ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.replace(-99, np.NaN, inplace=True)"
      ],
      "metadata": {
        "id": "3WtZtIQhXbzB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for missing values in total after replacing the -99's with NAN's\n",
        "\n",
        "missing_values(df_windTurbine)\n",
        "\n",
        "#impute the missing values"
      ],
      "metadata": {
        "id": "AG12C8FDXjFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(df_windTurbine == 0 ).sum(axis = 0)"
      ],
      "metadata": {
        "id": "BF0xY2a5hZNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning"
      ],
      "metadata": {
        "id": "ci0e5V4wPrZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputing Missing Values"
      ],
      "metadata": {
        "id": "3D0Dy4QyPsTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imputing missing values in numerical columns"
      ],
      "metadata": {
        "id": "4Fuii6sohtH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine['wind_speed(m/s)'].fillna(value = df_windTurbine['wind_speed(m/s)'].mean(), inplace = True) # Skewness is -0.052\n",
        "df_windTurbine['atmospheric_temperature(Â°C)'].fillna(value = df_windTurbine['atmospheric_temperature(Â°C)'].median(), inplace = True) # Skewness is -1.68\n",
        "df_windTurbine['shaft_temperature(Â°C)'].fillna(value = df_windTurbine['shaft_temperature(Â°C)'].median(), inplace = True) # Skewness is -2.54\n",
        "df_windTurbine['blades_angle(Â°)'].fillna(value = df_windTurbine['blades_angle(Â°)'].mean(), inplace = True) # Skewness is -0.653\n",
        "df_windTurbine['gearbox_temperature(Â°C)'].fillna(value = df_windTurbine['gearbox_temperature(Â°C)'].median(), inplace = True) # Skewness is 1.30\n",
        "df_windTurbine['engine_temperature(Â°C)'].fillna(value = df_windTurbine['engine_temperature(Â°C)'].median(), inplace = True) # Skewness is -3.99\n",
        "df_windTurbine['motor_torque(N-m)'].fillna(value = df_windTurbine['motor_torque(N-m)'].mean(), inplace = True) # Skewness is 0.030\n",
        "df_windTurbine['generator_temperature(Â°C)'].fillna(value = df_windTurbine['generator_temperature(Â°C)'].mean(), inplace = True) # Skewness is -0.200\n",
        "df_windTurbine['atmospheric_pressure(Pascal)'].fillna(value = df_windTurbine['atmospheric_pressure(Pascal)'].mean(), inplace = True) # Skewness is 0.074\n",
        "df_windTurbine['windmill_body_temperature(Â°C)'].fillna(value = df_windTurbine['windmill_body_temperature(Â°C)'].median(), inplace = True) # Skewness is -2.21\n",
        "df_windTurbine['wind_direction(Â°)'].fillna(value = df_windTurbine['wind_direction(Â°)'].mean(), inplace = True) # Skewness is 0.170\n",
        "df_windTurbine['resistance(ohm)'].fillna(value = df_windTurbine['resistance(ohm)'].mean(), inplace = True) # Skewness is -0.682\n",
        "df_windTurbine['rotor_torque(N-m)'].fillna(value = df_windTurbine['rotor_torque(N-m)'].median(), inplace = True) # Skewness is -1.04\n",
        "df_windTurbine['blade_length(m)'].fillna(value = df_windTurbine['blade_length(m)'].median(), inplace = True) # Skewness is -8.76\n",
        "df_windTurbine['blade_breadth(m)'].fillna(value = df_windTurbine['blade_breadth(m)'].mean(), inplace = True) # Skewness is -0.183\n",
        "df_windTurbine['windmill_height(m)'].fillna(value = df_windTurbine['windmill_height(m)'].mean(), inplace = True) # Skewness is -0.067\n",
        "df_windTurbine['windmill_generated_power(kW/h)'].fillna(value = df_windTurbine['windmill_generated_power(kW/h)'].mean(), inplace = True) # Skewness is 0.683"
      ],
      "metadata": {
        "id": "Ibv0W41via-N"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values(df_windTurbine)"
      ],
      "metadata": {
        "id": "2OEhHNSypGSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.isna().sum().sum()"
      ],
      "metadata": {
        "id": "CRS1EwtFsISf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Correlations"
      ],
      "metadata": {
        "id": "ErCVLZXz53gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure (figsize =(12,12))\n",
        "sns.heatmap(df_windTurbine.corr(),annot=True,cbar=0, linewidths=2,vmax=1, vmin=0, square=True,cmap ='Blues').set_title('HeatMap')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oXb4i3OV5175"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**:\n",
        "The feature \"generator_temperature(Â°C)\" has high correlation of 0.93 with feature \"motor_torque(N-m)\n",
        "\n",
        "It also has correlation greator than 0.5 with other two features.\n",
        "\n",
        "Therefore, drop the feature \"generator_temperature(Â°C)\" to avoid Multicolinearity."
      ],
      "metadata": {
        "id": "WJdZdAuR0EaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "dssxjDo1QrdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert the object column to datetime column \n",
        "df_windTurbine['datetime'] = df_windTurbine['datetime'].apply(pd.to_datetime)"
      ],
      "metadata": {
        "id": "trlvCN_QMInN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.info()"
      ],
      "metadata": {
        "id": "zFCof7_JMTba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting month and hour from datetime column\n",
        "df_windTurbine['month'] = pd.DatetimeIndex(df_windTurbine['datetime']).month\n",
        "df_windTurbine ['hour'] = pd.DatetimeIndex(df_windTurbine['datetime']).hour"
      ],
      "metadata": {
        "id": "wr8MC0wxMcTt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.info()"
      ],
      "metadata": {
        "id": "-i-CrynpMrA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.shape"
      ],
      "metadata": {
        "id": "hDqjcNzUtV_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the id and datetime columns\n",
        "\n",
        "df_windTurbine = df_windTurbine.drop(['tracking_id','datetime'],axis =1)"
      ],
      "metadata": {
        "id": "6SWuFjIVtU5d"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop \"generator_temperature(Â°C)\" to avoid multicolinearity\n",
        "#df_windTurbine = df_windTurbine.drop('generator_temperature(Â°C)',axis =1)"
      ],
      "metadata": {
        "id": "DSwlDx6X1aPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.info()"
      ],
      "metadata": {
        "id": "KeV-kOwwTjqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## imputing Missing categorical values with KNN Imputer "
      ],
      "metadata": {
        "id": "B1TCviloZTGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imputing the missing values in the categorical columns \"turbine_status and cloud_level\" \n",
        "# with mode will introduce the columns class. And therefore resulting bias in the stasistical analysis after imputation "
      ],
      "metadata": {
        "id": "IQsUTaJmx7EY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Categorical Columns\n",
        "df_objs = df_windTurbine.select_dtypes(include='object')\n",
        "df_objs.head(5)"
      ],
      "metadata": {
        "id": "7xCluhEoZRKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_objs.shape"
      ],
      "metadata": {
        "id": "L2HeExWXgMp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_objs.isna().sum()"
      ],
      "metadata": {
        "id": "gYMFkeBEW7mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first transform categorical features into numeric ones while preserving the\n",
        "# NaN values (LabelEncoder: that keeps missing values as 'NaN'), \n",
        "# then you can use the KNNImputer using only the nearest neighbour as replacement.\n",
        "# https://stackoverflow.com/questions/64900801/implementing-knn-imputation-on-categorical-variables-in-an-sklearn-pipeline\n",
        "\n",
        "# https://stackoverflow.com/questions/54444260/labelencoder-that-keeps-missing-values-as-nan\n",
        "# pd.Series(\n",
        "#     LabelEncoder().fit_transform(series[series.notnull()]),\n",
        "#     index=series[series.notnull()].index\n",
        "# )\n",
        "# series[series.notnull()] drop NaN values, then feeds the rest to the fit_transform.\n",
        "\n",
        "# as the label encoder returns a numpy.array and throws out an index, \n",
        "# index=series[series.notnull()].index restores it to concatenate it correctly. \n",
        "# If don't do indexing values shift from correct positions - and even an IndexError may occur.\n",
        "# Single encoder for all columns\n",
        "\n",
        "#In That case, stack dataframe, fit encode, then unstack it:\n",
        "\n",
        "\n",
        "# series_stack = df_objs.stack().astype(str)\n",
        "# label_encoder = LabelEncoder()\n",
        "# df_objs = pd.Series(\n",
        "#     label_encoder.fit_transform(series_stack),\n",
        "#     index=series_stack.index\n",
        "# ).unstack()\n",
        "# # print(df_objs)\n",
        "\n",
        "\n",
        "# as the series_stack is pd.Series containing NaN's, \n",
        "# all values from the DataFrame is floats, so you may prefer to convert it."
      ],
      "metadata": {
        "id": "yCzavrh1aAot"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the KNNImputer using only the nearest neighbour as replacement\n",
        "\n",
        "# from sklearn.impute import KNNImputer\n",
        "\n",
        "# imputer = KNNImputer()\n",
        "# # df_encoded = imputer.fit_transform(df_objs)\n",
        "\n",
        "# df_encoded = pd.DataFrame(np.round(imputer.fit_transform(df_objs)),columns = df_objs.columns)"
      ],
      "metadata": {
        "id": "PG-RF2ACbKfP"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_encoded.isna().sum()      #check if imputation is done correctly or not"
      ],
      "metadata": {
        "id": "ADiQ-TnWfa1U"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_encoded.shape"
      ],
      "metadata": {
        "id": "wt5hEmtudSqC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install fancyimpute"
      ],
      "metadata": {
        "id": "V5B7Z6QmhE3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fancyimpute import KNN\n",
        "\n",
        "#instantiate both packages to use\n",
        "encoder = LabelEncoder()\n",
        "imputer = KNN()\n",
        "# create a list of categorical columns to iterate over\n",
        "cat_cols = ['turbine_status','cloud_level']\n",
        "\n",
        "def encode(data):\n",
        "    '''function to encode non-null data and replace it in the original data'''\n",
        "    #retains only non-null values\n",
        "    nonulls = np.array(data.dropna())\n",
        "    #reshapes the data for encoding\n",
        "    impute_reshape = nonulls.reshape(-1,1)\n",
        "    #encode date\n",
        "    impute_label = encoder.fit_transform(impute_reshape)\n",
        "    #Assign back encoded values to non-null values\n",
        "    data.loc[data.notnull()] = np.squeeze(impute_label)\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "7s5tcNfEZ5q_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_objs.shape"
      ],
      "metadata": {
        "id": "xvl4z_s8jAdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #create a for loop to iterate through each column in the data\n",
        "for columns in cat_cols:\n",
        "    encode(df_objs[columns])"
      ],
      "metadata": {
        "id": "Hg3Be5GkhKSN"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_objs.shape"
      ],
      "metadata": {
        "id": "PV7TSQZ4hqgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  impute data and convert \n",
        "df_encoded = pd.DataFrame(np.round(imputer.fit_transform(df_objs)),columns = df_objs.columns)"
      ],
      "metadata": {
        "id": "3uYTdrwdhM-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded.shape"
      ],
      "metadata": {
        "id": "cekNbFpThWMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop th original \"turbine_status\" and\t\"cloud_level\" columns from the dataset. \n",
        "# concatenate the encoded categorical columns. and apply KNNImputer to impute the missing values."
      ],
      "metadata": {
        "id": "dbFkK1oTcPdD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the original columns\n",
        "df_windTurbine = df_windTurbine.drop(['turbine_status', 'cloud_level'],axis =1)"
      ],
      "metadata": {
        "id": "o_e6A8ADRHs8"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_windTurbine.columns"
      ],
      "metadata": {
        "id": "a5xcgAoaRSSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the encoded categorical columns\n",
        "df = pd.concat([df_windTurbine,df_encoded],axis=1)"
      ],
      "metadata": {
        "id": "OLgs0GQZRJ_I"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "x-sHYMCEbFFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values(df)"
      ],
      "metadata": {
        "id": "2ssE_rTRbPsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section7></a>\n",
        "# **7 Data Post-Profiling**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ihGDKykpQm6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas profiling\n",
        "\n",
        "from pandas_profiling import ProfileReport                          # Import Pandas Profiling (To generate Univariate Analysis) \n",
        "windTurbine_post_profile = ProfileReport(df)\n",
        "windTurbine_post_profile"
      ],
      "metadata": {
        "id": "fMgvrIvwsi7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section9></a>\n",
        "# **9 Splitting Data Into Train and Test**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "DW4mMhMdQsMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate out the data into X features and y labels\n",
        "# \"Target\" is the dependent variable\n",
        "\n",
        "X = df.drop('windmill_generated_power(kW/h)',axis=1)\n",
        "y = df['windmill_generated_power(kW/h)']"
      ],
      "metadata": {
        "id": "KPARuHNVljsM"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split up X and y into a training set and test set.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
      ],
      "metadata": {
        "id": "odYL8iSTubi9"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('x_train shape: ', X_train.shape)\n",
        "print('y_train shape: ', y_train.shape)\n",
        "print('x_test shape: ', X_test.shape)\n",
        "print('y_test shape: ', y_test.shape)"
      ],
      "metadata": {
        "id": "jfTj7tIiuh-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section10></a>\n",
        "# **10 Scaling / Standardizing**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yXmIek83Qstz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the X features\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "0jMITXvBlqla"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_X_train = scaler.fit_transform(X_train)\n",
        "scaled_X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "bWYBPQqKlz_p"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section11></a>\n",
        "# **11 Model Building**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6nMoZD26QtDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor \n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost.sklearn import XGBRegressor"
      ],
      "metadata": {
        "id": "0taawdoAn_Za"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Multiple Base Model"
      ],
      "metadata": {
        "id": "kp2elq4FRc2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clfs = [LinearRegression(fit_intercept=True,),DecisionTreeRegressor(random_state = 42), RandomForestRegressor (n_estimators = 100, random_state = 42, n_jobs = -1)\n",
        "       ,KNeighborsRegressor(n_neighbors = 6),GaussianProcessRegressor(), GradientBoostingRegressor(random_state = 42),XGBRegressor(n_estimators=64, random_state=42)]"
      ],
      "metadata": {
        "id": "bVI-bkPwnaS0"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for clf in clfs:\n",
        "\n",
        "  # Extracting model name\n",
        "  model_name = type(clf).__name__\n",
        "\n",
        "  # Calculate start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  # Train the model\n",
        "  clf.fit(scaled_X_train, y_train)\n",
        "\n",
        "  # Make predictions on the trained model\n",
        "  predictions = clf.predict(scaled_X_test)\n",
        "\n",
        "\n",
        "  RMSE = np.sqrt(metrics.mean_squared_error(y_test, predictions))\n",
        "  R_squared = metrics.r2_score(y_test, predictions)\n",
        "\n",
        "\n",
        "  # Calculate evaluated time\n",
        "  elapsed_time = (time.time() - start_time)\n",
        "\n",
        "  # Display the metrics and time took to develop the model\n",
        "  print('Performance Metrics of', model_name, ':')  \n",
        "  print('[Processing Time]:', elapsed_time, 'seconds') \n",
        "\n",
        "  print('[RMSE Value]:', RMSE)\n",
        "  print('[R2 Score]:',  R_squared)\n",
        "  print('----------------------------------------\\n')"
      ],
      "metadata": {
        "id": "s8qU65y6nP3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**:\n",
        "\n",
        "**Multiple Baseline Model Performance** with default parameters:\n",
        "\n",
        "| Model Name |R2-Score|\n",
        "|--|--|\n",
        "|**LogisticRegression :**|46.01|\n",
        "|**Decision Tree Regressor :**|89.03|\n",
        "|**Random Forest Regressor :**|94.53|\n",
        "|**K Neighbour Regressor :**|57.85|\n",
        "|**Gaussian N B :**|-1.82|\n",
        "|**Gradient Boosting Regressor :**|93.49|\n",
        "|**XGBRegressor :**|92.98|\n",
        "\n",
        "\n",
        "**Clearly Random Forest Regressor performs better than other Classifiers.**"
      ],
      "metadata": {
        "id": "Pn9HI8QGvGX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning "
      ],
      "metadata": {
        "id": "ytmZwy0FRdYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Regressor\n",
        " \n",
        "rf_model = RandomForestRegressor(n_estimators=500, n_jobs= -1, random_state=42)\n",
        "rf_model.fit(scaled_X_train, y_train)\n",
        "\n",
        "y_pred_train_rf = rf_model.predict(scaled_X_train)\n",
        "\n",
        "y_pred_test_rf = rf_model.predict(scaled_X_test)\n",
        "\n",
        "RMSE_train_rf = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_rf))\n",
        "r2_train_rf = metrics.r2_score(y_train, y_pred_train_rf)\n",
        "\n",
        "RMSE_test_rf = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_rf))\n",
        "r2_test_rf = metrics.r2_score(y_test, y_pred_test_rf)\n",
        "\n",
        "print('Random Forest Regressor:')\n",
        "print('RMSE for training set is {}'.format(RMSE_train_rf))\n",
        "print('R2 for training set is {}'.format(r2_train_rf))\n",
        "\n",
        "print('RMSE for testing set is {}'.format(RMSE_test_rf))\n",
        "print('R2 for testing set is {}'.format(r2_test_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1zXeSbCzQBO",
        "outputId": "0736213d-8d27-491d-8cce-e9914b31b996"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor:\n",
            "RMSE for training set is 0.22690856023656067\n",
            "R2 for training set is 0.9927789844443913\n",
            "RMSE for testing set is 0.5955505960893106\n",
            "R2 for testing set is 0.9498974938824949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Regressor\n",
        "\n",
        "gbr=GradientBoostingRegressor(n_estimators=500, random_state=42)\n",
        "gbr.fit(scaled_X_train,y_train)\n",
        "\n",
        "y_pred_train_gbr =gbr.predict(scaled_X_train) \n",
        " \n",
        "y_pred_test_gbr = gbr.predict(scaled_X_test)\n",
        "\n",
        "RMSE_train_gbr = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_gbr))\n",
        "r2_train_gbr = metrics.r2_score(y_train, y_pred_train_gbr)\n",
        "\n",
        "RMSE_test_gbr = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_gbr))\n",
        "r2_test_gbr = metrics.r2_score(y_test, y_pred_test_gbr)\n",
        "\n",
        "print('Gradient Boosting Regressor:')\n",
        "print('RMSE for training set is {}'.format(RMSE_train_gbr))\n",
        "print('R2 for training set is {}'.format(r2_train_gbr))\n",
        "\n",
        "print('RMSE for testing set in LR is {}'.format(RMSE_test_gbr))\n",
        "print('R2 for testing set in LR is {}'.format(r2_test_gbr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAM8XPIl2Fxh",
        "outputId": "35da89b9-21d7-4531-a033-0fd322b92916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor:\n",
            "RMSE for training set is 0.4604474896545634\n",
            "R2 for training set is 0.9702657670645427\n",
            "RMSE for testing set in LR is 0.6123777664794735\n",
            "R2 for testing set in LR is 0.947026221500934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost Regressor\n",
        "\n",
        "xgb_model = XGBRegressor(n_estimators=500, n_jobs= -1, random_state=42)\n",
        "xgb_model.fit(scaled_X_train, y_train)\n",
        "\n",
        "y_pred_train_eb = xgb_model.predict(scaled_X_train)\n",
        "\n",
        "y_pred_test_eb = xgb_model.predict(scaled_X_test)\n",
        "\n",
        "RMSE_train_eb = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_eb))\n",
        "r2_train_eb = metrics.r2_score(y_train, y_pred_train_eb)\n",
        "\n",
        "print('RMSE for training set in XGB is {}'.format(RMSE_train_eb))\n",
        "print('R2 for training set in LR XGB {}'.format(r2_train_eb))\n",
        "\n",
        "RMSE_test_eb = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_eb))\n",
        "r2_test_eb = metrics.r2_score(y_test, y_pred_test_eb)\n",
        "\n",
        "print('RMSE for testing set in XGB is {}'.format(RMSE_test_eb))\n",
        "print('R2 for testing set in XGB is {}'.format(r2_test_eb))"
      ],
      "metadata": {
        "id": "F-b4Yhwnn6QO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Model Name |R2-Score - train set|R2-Score - test set|\n",
        "|--|--|--|\n",
        "|**Random Forest Regressor :**|99.27|94.98|\n",
        "|**Gradient Boosting Regressor :**|97.02|94.70|\n",
        "|**XGBRegressor :**|96.77|94.67|"
      ],
      "metadata": {
        "id": "QwbJvZPTwWoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tunung - Gradient Boosting Regressor Model -1"
      ],
      "metadata": {
        "id": "KHIBU93wReEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypertuned Model -2"
      ],
      "metadata": {
        "id": "WSo4VTAdRgB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model=GradientBoostingRegressor()\n",
        "params={'n_estimators':range(1,200)}\n",
        "grid=GridSearchCV(estimator=model,cv=2,param_grid=params,scoring='r2')\n",
        "grid.fit(scaled_X_train,y_train)\n",
        "print(\"The best estimator returned by GridSearch CV is:\",grid.best_estimator_)\n",
        "\n"
      ],
      "metadata": {
        "id": "vgmhEaGUHoWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_gbr =grid.predict(scaled_X_train) \n",
        "\n",
        "RMSE_train_gbr = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train_gbr))\n",
        "r2_train_gbr = metrics.r2_score(y_train, y_pred_train_gbr)\n",
        "\n",
        "print('Gradient Boosting Regressor:')\n",
        "print('RMSE for training set is {}'.format(RMSE_train_gbr))\n",
        "print('R2 for training set is {}'.format(r2_train_gbr))"
      ],
      "metadata": {
        "id": "qmsMgupDJgE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The best estimator returned by GridSearch CV is:  GradientBoostingRegressor(n_estimators=199)\n",
        "GB=grid.best_estimator_\n",
        "y_pred_test_gbr = GB.predict(scaled_X_test)\n",
        "\n",
        "RMSE_test_gbr = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test_gbr))\n",
        "r2_test_gbr = metrics.r2_score(y_test, y_pred_test_gbr)\n",
        "\n",
        "print('RMSE for testing set in LR is {}'.format(RMSE_test_gbr))\n",
        "print('R2 for testing set in LR is {}'.format(r2_test_gbr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvfA0Z4JIZLs",
        "outputId": "ca339cf6-2204-4c7e-f8d2-24f120d62601"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE for testing set in LR is 0.6282342841698788\n",
            "R2 for testing set in LR is 0.9442571431317365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection\n",
        "\n",
        "The best estimator is: \n",
        "Random forest Regressor(n_estimators=500,n_jobs = -1,random_state = 42)\n",
        "\n",
        "The train R2 Score is: 99.24\n",
        "\n",
        "The test R2 Score is: 94.66"
      ],
      "metadata": {
        "id": "KzrK5WxARgfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation\n",
        "Lets evaluate the unseen data with best estimator:\n",
        "Random forest Regressor(n_estimators=500,n_jobs = -1,random_state = 42)"
      ],
      "metadata": {
        "id": "RCkm2R-XRg7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section12></a>\n",
        "# **12 Predicting Unseen Data**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zSrc7ctyRhaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unseen Data Fetching"
      ],
      "metadata": {
        "id": "4mRn2Xm0Sz4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_unseen = pd.read_csv(\"/content/drive/MyDrive/Windturbine_test.csv\")"
      ],
      "metadata": {
        "id": "WS4laZpiP1Q6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unseen.head(5)"
      ],
      "metadata": {
        "id": "s2aRxZJNP-25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unseen.info()"
      ],
      "metadata": {
        "id": "7EinwDk1QHLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unseen.describe()"
      ],
      "metadata": {
        "id": "Xt0_le4IQJKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unseen Data Preprocessing"
      ],
      "metadata": {
        "id": "0i5--V5tS0cQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Feature Engineering - remove id column, extract time and month  columns from datetime. and drop datetime column\n",
        "\n",
        "#convert the object column to datetime column \n",
        "df_unseen['datetime'] = df_unseen['datetime'].apply(pd.to_datetime)\n",
        "\n",
        "#Extracting month and hour from datetime column\n",
        "df_unseen['month'] = pd.DatetimeIndex(df_unseen['datetime']).month\n",
        "df_unseen ['hour'] = pd.DatetimeIndex(df_unseen['datetime']).hour"
      ],
      "metadata": {
        "id": "Z58OMoqJW3xI"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop \"generator_temperature(Â°C)\" to avoid multicolinearity\n",
        "# df_unseen = df_unseen.drop('generator_temperature(Â°C)',axis =1)"
      ],
      "metadata": {
        "id": "Fqvandqm-FkV"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_submission = df_unseen['tracking_id']\n",
        "data_submission.head()"
      ],
      "metadata": {
        "id": "tCxf6UfofKAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the id column\n",
        "df_unseen = df_unseen.drop(['tracking_id','datetime'],axis =1)"
      ],
      "metadata": {
        "id": "wq9U4B19gMwv"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unseen.info()"
      ],
      "metadata": {
        "id": "ojoS1apPXacj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check missing  values\n",
        "missing_values(df_unseen)"
      ],
      "metadata": {
        "id": "vfnKMHGtQL9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check ALL the NUMERICAL COLUMNS for -99 values and Replace/Substitute them with NaN values\n",
        "\n",
        "(df_unseen == -99 ).sum(axis = 0)"
      ],
      "metadata": {
        "id": "M9ivH0Q_yqkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unseen.replace(-99, np.NaN, inplace=True)"
      ],
      "metadata": {
        "id": "RQNglghtYWOP"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#impute the missing values\n",
        "\n",
        "(df_unseen == 0 ).sum(axis = 0)"
      ],
      "metadata": {
        "id": "8I7WavhmyvM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unseen.isna().sum()"
      ],
      "metadata": {
        "id": "lFUxZkguYePv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check duplicates \n",
        "duplicate_values(df_unseen)"
      ],
      "metadata": {
        "id": "-9vaFiAjQO4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# impute numerical missing values "
      ],
      "metadata": {
        "id": "NuP5KE5xQS8g"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_unseen['wind_speed(m/s)'].fillna(value = df_unseen['wind_speed(m/s)'].mean(), inplace = True) # Skewness is -0.052\n",
        "df_unseen['atmospheric_temperature(Â°C)'].fillna(value = df_unseen['atmospheric_temperature(Â°C)'].median(), inplace = True) # Skewness is -1.68\n",
        "df_unseen['shaft_temperature(Â°C)'].fillna(value = df_unseen['shaft_temperature(Â°C)'].median(), inplace = True) # Skewness is -2.54\n",
        "df_unseen['blades_angle(Â°)'].fillna(value = df_unseen['blades_angle(Â°)'].mean(), inplace = True) # Skewness is -0.653\n",
        "df_unseen['gearbox_temperature(Â°C)'].fillna(value = df_unseen['gearbox_temperature(Â°C)'].median(), inplace = True) # Skewness is 1.30\n",
        "df_unseen['engine_temperature(Â°C)'].fillna(value = df_unseen['engine_temperature(Â°C)'].median(), inplace = True) # Skewness is -3.99\n",
        "df_unseen['motor_torque(N-m)'].fillna(value = df_unseen['motor_torque(N-m)'].mean(), inplace = True) # Skewness is 0.030\n",
        "df_unseen['generator_temperature(Â°C)'].fillna(value = df_unseen['generator_temperature(Â°C)'].mean(), inplace = True) # Skewness is -0.200\n",
        "df_unseen['atmospheric_pressure(Pascal)'].fillna(value = df_unseen['atmospheric_pressure(Pascal)'].mean(), inplace = True) # Skewness is 0.074\n",
        "df_unseen['windmill_body_temperature(Â°C)'].fillna(value = df_unseen['windmill_body_temperature(Â°C)'].median(), inplace = True) # Skewness is -2.21\n",
        "df_unseen['wind_direction(Â°)'].fillna(value = df_unseen['wind_direction(Â°)'].mean(), inplace = True) # Skewness is 0.170\n",
        "df_unseen['resistance(ohm)'].fillna(value = df_unseen['resistance(ohm)'].mean(), inplace = True) # Skewness is -0.682\n",
        "df_unseen['rotor_torque(N-m)'].fillna(value = df_unseen['rotor_torque(N-m)'].median(), inplace = True) # Skewness is -1.04\n",
        "df_unseen['blade_length(m)'].fillna(value = df_unseen['blade_length(m)'].median(), inplace = True) # Skewness is -8.76\n",
        "df_unseen['blade_breadth(m)'].fillna(value = df_unseen['blade_breadth(m)'].mean(), inplace = True) # Skewness is -0.183\n",
        "df_unseen['windmill_height(m)'].fillna(value = df_unseen['windmill_height(m)'].mean(), inplace = True) # Skewness is -0.067\n"
      ],
      "metadata": {
        "id": "PBiXHPaNy-iM"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values(df_unseen)"
      ],
      "metadata": {
        "id": "XFISDlcEWbbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# impute categorical missing values Categorical Columns of unseen/test data\n",
        "\n",
        "df_objs_test = df_unseen.select_dtypes(include='object')\n",
        "df_objs_test.head(5)"
      ],
      "metadata": {
        "id": "GzVzsNsWVXFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_objs_test.isna().sum()"
      ],
      "metadata": {
        "id": "Xh0QhrDbcQSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instantiate both packages to use\n",
        "encoder = LabelEncoder()\n",
        "imputer = KNN()\n",
        "# create a list of categorical columns to iterate over\n",
        "cat_cols = ['turbine_status','cloud_level']\n",
        "\n",
        "def encode(data):\n",
        "    '''function to encode non-null data and replace it in the original data'''\n",
        "    #retains only non-null values\n",
        "    nonulls = np.array(data.dropna())\n",
        "    #reshapes the data for encoding\n",
        "    impute_reshape = nonulls.reshape(-1,1)\n",
        "    #encode date\n",
        "    impute_label = encoder.fit_transform(impute_reshape)\n",
        "    #Assign back encoded values to non-null values\n",
        "    data.loc[data.notnull()] = np.squeeze(impute_label)\n",
        "    return data\n",
        "\n",
        "# #create a for loop to iterate through each column in the data\n",
        "for columns in cat_cols:\n",
        "    encode(df_objs_test[columns])"
      ],
      "metadata": {
        "id": "zPVuuY0YT-UK"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_objs_test.isna().sum()"
      ],
      "metadata": {
        "id": "rhTEEQ1WXmm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # impute data and convert \n",
        "df_encoded_test = pd.DataFrame(np.round(imputer.fit_transform(df_objs_test)),columns = df_objs_test.columns)"
      ],
      "metadata": {
        "id": "Nubiqmq-Vj4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded_test.isna().sum()"
      ],
      "metadata": {
        "id": "xb2t59U1cEac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the original columns\n",
        "df_unseen = df_unseen.drop(['turbine_status', 'cloud_level'],axis =1)"
      ],
      "metadata": {
        "id": "A_uLfaEjVonG"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the encoded categorical columns\n",
        "df_final = pd.concat([df_unseen,df_encoded_test],axis=1)"
      ],
      "metadata": {
        "id": "lDMuu39YVq1H"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values(df_final)"
      ],
      "metadata": {
        "id": "ZAuBpdkNVuBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unseen Data Prediction"
      ],
      "metadata": {
        "id": "OmvL5MfsTAPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "scaled_arr_final = scaler.fit_transform(df_final)\n",
        "\n",
        "# Inputting our transformed data in a dataframe\n",
        "data_final_model = pd.DataFrame(data=scaled_arr_final, columns=df_final.columns)\n",
        "\n",
        "# Getting a glimpse of transformed data\n",
        "data_final_model.head()"
      ],
      "metadata": {
        "id": "OpBHM2Txf-wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_final_model.info()"
      ],
      "metadata": {
        "id": "DEVE446BhI6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict the unseen data\n",
        "y_pred_test_rfc = rf_model.predict(data_final_model)\n"
      ],
      "metadata": {
        "id": "e9P0MMX-c5lF"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert the array into a DataFrame\n",
        "\n",
        "y_pred_test_rfc = pd.DataFrame(y_pred_test_rfc)"
      ],
      "metadata": {
        "id": "-PADQpnxiCbe"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test_rfc"
      ],
      "metadata": {
        "id": "i_8e2jHpidtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test_rfc.nunique()"
      ],
      "metadata": {
        "id": "NCroD9bdimHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section13></a>\n",
        "# **13 Preparing Submission File**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Ih0ouJTMRhdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission_file = pd.concat([data_submission,y_pred_test_rfc], axis = 1)"
      ],
      "metadata": {
        "id": "nb99grjzii2s"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_file.head(20)"
      ],
      "metadata": {
        "id": "igCjgb2Ui4B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To write the final data to the submission file which is .csv without HEADER and INDEX"
      ],
      "metadata": {
        "id": "Y9OLZchji_6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installed the s3fs package using pip\n",
        "!pip3 install s3fs"
      ],
      "metadata": {
        "id": "UpMHR2PHi7-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_file.to_csv('D://Wind_Turbine_Prediction_Submission.csv', header=False, index=False)"
      ],
      "metadata": {
        "id": "CoJCI8HBjHCF"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "<a name = Section14></a>\n",
        "# **14 Summary**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "w7hArvBFRiCX"
      }
    }
  ]
}